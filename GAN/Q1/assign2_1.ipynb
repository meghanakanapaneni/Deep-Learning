{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2a.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GXRoPAtJFMQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "262e038d-a64a-4652-a8a9-a2cbe9376df3"
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "        print(\"***********\",latent_dim)\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.avgpool =  nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        print(\"----\",int(np.prod(img_shape)))\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        \n",
        "        img_flat = img.view(img.shape[0], -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "def define_dataset():\n",
        "\troot_dir = 'drive/My Drive/SAMPLE/faces94'\n",
        "\tfiles = glob(f\"{root_dir}/**/**/*.jpg\")\n",
        "\n",
        "\tlab = {\n",
        "\t    \"female\": 0,\n",
        "\t    \"male\":1,\n",
        "\t    \"malestaff\": 2 \n",
        "\t}\n",
        "\treturn files,lab\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "img_shape = (3, 200, 180)\n",
        "interval = 100\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "latent_dim = 200\n",
        "\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "files,lab=define_dataset()\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.FloatTensor\n",
        "\n",
        "\n",
        "\n",
        "transform_loader = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5],[0.5])\n",
        "])\n",
        "\n",
        "all_imgs = torch.stack([transform_loader(Image.open(x)) for x in files])\n",
        "all_label = torch.tensor([lab[x.split('/')[-3]] for x in files])\n",
        "\n",
        "train_idx, test_idx = train_test_split(range(len(all_imgs)), test_size=0.2, random_state=102)\n",
        "\n",
        "train_img = all_imgs[train_idx]\n",
        "train_label = all_label[train_idx]\n",
        "\n",
        "test_img = all_imgs[test_idx]\n",
        "test_label = all_label[test_idx]\n",
        "\n",
        "train_data = TensorDataset(train_img, train_label)\n",
        "test_data = TensorDataset(test_img, test_label)\n",
        "\n",
        "train_samp = RandomSampler(train_data)\n",
        "test_samp = SequentialSampler(test_data)\n",
        "\n",
        "train_loader = DataLoader(train_data, sampler=train_samp, batch_size=32)\n",
        "test_loader = DataLoader(test_data, sampler=test_samp, batch_size=32)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs,_) in enumerate(train_loader):\n",
        "      \n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "        \n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "       \n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        gen_imgs = generator(z)\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, epochs, i, len(train_loader), d_loss.item(), g_loss.item())\n",
        "        )\n",
        "\n",
        "        batches_done = epoch * len(train_loader) + i\n",
        "        if batches_done % interval == 0:\n",
        "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "*********** 200\n",
            "---- 108000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}